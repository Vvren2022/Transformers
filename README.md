# Transformers
✅ Handles Long-Range Dependencies – Unlike RNNs, transformers process entire sequences at once, making them efficient for long texts, images, and time-series data.  ✅ Self-Attention Mechanism – Allows the model to focus on the most relevant parts of input, improving context understanding
